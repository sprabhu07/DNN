{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:\n",
    "\n",
    "Description:\n",
    "Logistic regression[1] tries to classify by the probability returned by a sigmoid function (the hypothesis \n",
    "in this case) as below: \n",
    "            hyp(X.W) = hyp(z) = 1/(1 + exp(-z)) \n",
    "            where X = vector of input features  \n",
    "            W = vector of weights for corresponding feature of X \n",
    "            z = product of theta and X = W0 + W1.x1+W2.x2+‚Ä¶Wn.xn \n",
    "            where x1,x2,x3‚Ä¶xn are feature values  \n",
    "The algorithm tries to classify one class from another by this probability. For example, one can say \n",
    "the prediction belongs to class 1 when the probability is greater than or equal to 0.5 and belongs to \n",
    "class 0 when the probability is less than 0.5. \n",
    "            The cost function comes out to be: \n",
    "            cost = (-log(hyp(z)).y) - log(1-hyp(z)).(1-y)))  \n",
    "            where y = actual value                          \n",
    "            hyp(z) = probability given by the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LogisticRegression(BaseEstimator): # implemented class for the model\n",
    "   \"\"\"A class that does performs logistic regression.\"\"\"\n",
    "   \n",
    "   def __init__(self,W=[],b=[],learningFactor = 0.05,iterations=1000): # some parameters can be set\n",
    "      \"\"\"Initializes values.Takes in the supplied learningFactor and the iterations\"\"\"\n",
    "      self.learningFactor = learningFactor                             # set the learning factor\n",
    "      self.iterations = iterations                                     # set the iterations\n",
    "      self.X = X                                                       # set the X value\n",
    "      self.y = y                                                       # set the y value\n",
    "      self.W = W\n",
    "      self.b = b\n",
    "      self.v_delW = []\n",
    "      self.v_delb = []\n",
    "      self.class_labels = []\n",
    "\n",
    "   def sigmoid(self,z): # code for the sigmoid function\n",
    "      \"\"\"This function returns the sigmoid value of the input z\"\"\"\n",
    "      return (1/(1+np.exp(-z))) \n",
    "\n",
    "   \n",
    "\n",
    "   def costFunctionAndGradient(self,x,y,W_learn,b_learn):\n",
    "      \"\"\"This function returns the costFunctionAndGradient\"\"\"\n",
    "      a = self.sigmoid(np.dot(x,W_learn.T) + b_learn)               # calculate the value of hypothesis\n",
    "      cost = (-np.dot(np.log(a),y.T) - np.dot(np.log(1-a),(1-y).T)) # calculate cost for Stochastic Gradient descent\n",
    "      gradient_W = np.dot((a-y),x)                                  # calculate the gradient for Weights W\n",
    "      gradient_b = a-y                                              # calculate the gradient for bias b\n",
    "      return cost,gradient_W,gradient_b\n",
    "\n",
    "   def gradientDescent(self,x,y,W_learn,b_learn):                   # code for the Gradient Descent algorithm\n",
    "      \"\"\"This function does Gradient Descent and learns the weight values\"\"\"\n",
    "      costValue,gradient_W,gradient_b = self.costFunctionAndGradient(x,y,W_learn,b_learn)\n",
    "      self.W = self.W - self.learningFactor * gradient_W            # update the weight values in every iteration\n",
    "      self.b = self.b - self.learningFactor * gradient_b            # update the bias values in every iteration\n",
    "      return costValue\n",
    "        \n",
    "   def fit(self,X,y): # the fit function which takes in the features & output vectors\n",
    "      \"\"\"This function helps the model learn from the given labelled examples\"\"\"\n",
    "      self.X = X\n",
    "      self.y = y\n",
    "      seed = 1\n",
    "      rows,columns = X.shape\n",
    "      self.W = np.random.random(columns) # start from random values for weights\n",
    "      self.b = np.random.random()\n",
    "      self.convertLabels(list(set(y)))   # identify all unique labels\n",
    "      for i in range(self.iterations):\n",
    "        sample = np.random.randint(len(X))\n",
    "        self.gradientDescent(X[sample],y[sample],self.W,self.b) # save the learned weight values\n",
    "   \n",
    "   def convertLabels(self,labels):\n",
    "        self.class_labels = { 0 : labels[0] , 1 : labels[1]}\n",
    "\n",
    "   def predict(self,X): # function to predict class for given feature vector X\n",
    "      \"\"\"This function predicts the class for the given features X\"\"\"\n",
    "      x = np.array(X)\n",
    "      #print(self.b)  \n",
    "      a = self.sigmoid(np.dot(x,self.W.T) + self.b)\n",
    "      #print(f\"Prediction: {a}\")\n",
    "      if a >= 0.5:\n",
    "         return self.class_labels[1]            # choose the class which returns highest value/probability for our hypothesis\n",
    "      return self.class_labels[0]               # choose the other class\n",
    "\n",
    "   def score(self,X,y):                         # function which returns the accuracy value\n",
    "      \"\"\"This function returns the accuracy score for the X values and compares them with output y.\"\"\"\n",
    "      count = 0  \n",
    "      for x,y_ in zip(X,y):\n",
    "        predictedLabel = self.predict(x)\n",
    "        if predictedLabel == y_:\n",
    "            count += 1\n",
    "      return count/len(y)                       # calculate accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:\n",
    "\n",
    "Moons dataset:\n",
    "On this dataset, I got a mean test accuracy of 0.8583333333333334 as seen below. From looking at the plot of the data,it is not linearly seperable.\n",
    "\n",
    "\n",
    "Blobs dataset:\n",
    "On this dataset, I got a mean test accuracy of 1.0 as seen below. From looking at the plot of the data,it is linearly seperable and can be easily classified into two distinguishable classes. This explains the high test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is  0.85\n",
      "The accuracy is  0.8833333333333333\n",
      "The accuracy is  0.9333333333333333\n",
      "The accuracy is  0.7833333333333333\n",
      "The accuracy is  0.8833333333333333\n",
      "The accuracy is  0.8833333333333333\n",
      "The accuracy is  0.9\n",
      "The accuracy is  0.8\n",
      "The accuracy is  0.85\n",
      "The accuracy is  0.8166666666666667\n",
      "The average accuracy is 0.8583333333333334\n"
     ]
    }
   ],
   "source": [
    "# Check Logistic regression for Moons data\n",
    "df1 = pd.read_csv('moons400.csv',sep=',')\n",
    "features = df1.drop(columns=['Class'])\n",
    "classes = df1['Class']\n",
    "X = np.array(features)\n",
    "y = np.array(classes)\n",
    "accuracyValues = []\n",
    "parameters = {'learningFactor' : [1e-3,1e-2,5e-3,5e-2],'iterations':[100,500,1000,2000]}    # draw up a list of hyperparameters\n",
    "#gs = GridSearchCV(estimator=LogisticRegression(),param_grid=parameters)                    # do a grid search\n",
    "#gs.fit(X,y)                                                                                # fit to the model\n",
    "#print(gs.best_estimator_)                                                                  # see the best model/parameters\n",
    "for i in range(10):                                                                     # use 10 models\n",
    "   X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=i) # use 15% of data as the test set\n",
    "   X_val,X_test,y_val,y_test = train_test_split(X_test, y_test,test_size=0.5,random_state=i)\n",
    "   lr = LogisticRegression()                                           # create an object\n",
    "   lr.fit(X_train,y_train)                                                              # train the model\n",
    "   score = lr.score(X_test,y_test)\n",
    "   print('The accuracy is ', score)                                                     # get the accuracy/score of the given inputs\n",
    "   accuracyValues.append(score)\n",
    "print('The average accuracy is',np.mean(accuracyValues))                                # get the mean accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The average accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check Logistic regression for blobs data\n",
    "df2 = pd.read_csv('blobs250.csv',sep=',')\n",
    "features = df2.drop(columns=['Class'])\n",
    "classes = df2['Class']\n",
    "X = np.array(features)\n",
    "y = np.array(classes)\n",
    "accuracyValues = []\n",
    "parameters = {'learningFactor' : [1e-3,1e-2,5e-3,5e-2],'iterations':[100,500,1000,2000]}    # draw up a list of hyperparameters\n",
    "#gs = GridSearchCV(estimator=LogisticRegression(),param_grid=parameters)                    # do a grid search\n",
    "#gs.fit(X,y)                                                                                # fit to the model\n",
    "#print(gs.best_estimator_)                                                                  # see the best model/parameters\n",
    "for i in range(10):                                                                         # use 10 models\n",
    "   X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=i) # use 15% of data as the test set\n",
    "   X_val,X_test,y_val,y_test = train_test_split(X_test, y_test,test_size=0.5,random_state=i)\n",
    "   lr = LogisticRegression(learningFactor=0.01,iterations=100)                              # create an object\n",
    "   lr.fit(X_train,y_train)                                                                  # train the model\n",
    "   score = lr.score(X_test,y_test)\n",
    "   print('The accuracy is ', score)                                                    # get the accuracy/score of the given inputs\n",
    "   accuracyValues.append(score)\n",
    "print('The average accuracy is',np.mean(accuracyValues))                               # get the mean accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implement Shallow NN\n",
    "\n",
    "Description:\n",
    "Shallow Neural Networks [2] are Neural Networks with a maximum of two hidden layers. This in contrast to Deep NNs where usually there are more than two hidden layers (and hence the name 'Deep'). For the assignment, I have implemented a Shallow NN with one hidden layer and output layer. The input data points (X) are passed to this hidden layer in a linear combination of weights and bias.For example:<br>\n",
    "     z1 = ùëä1,1 * ùë•1 + ùëä1,2 * ùë•2 + ùëä1,3 * ùë•3 + ùëè1 <br>\n",
    "     where z1 = linear combination that goes into one of the node in the hidden layer <br>\n",
    "           Wi,j = combination of weights for the xi inputs <br>\n",
    "           bi = bias <br>\n",
    "Each node in the hidden layer will then activate this linear combination as: <br>\n",
    "     a1 = f(ùëä1,1 * ùë•1 + ùëä1,2 * ùë•2 + ùëä1,3 * ùë•3 + ùëè1)                      <br>\n",
    "     where a1 = value of activation                                         <br>\n",
    "           f = the activation function                                      <br>\n",
    "In my case, I have chosen the activation function as sigmoid for all the layers. The model learns in the simple steps as below:\n",
    "1) We randomly intialise the weight and bias values\n",
    "2) The forward propagation: For all the layers,we calculate the activation value and pass it on to the next layer.\n",
    "3) We compare the final output of our network to the actual value and then backpropagate back the errors into the network.This means that we readjust our weights so that they are more closer to the output.\n",
    "4) We do this until the weights no longer change or till we have minimized the loss function. \n",
    "\n",
    "Blobs dataset:\n",
    "On this dataset, I got a mean test accuracy of 1.0 as seen below.This is similar to the Logistic Regression tests in part 2. From looking at the plot of the data,it is linearly seperable and can be easily classified into two distinguishable classes. This explains the high test accuracy.\n",
    "\n",
    "Moons dataset:\n",
    "From looking at the plot of the data,it is not linearly seperable as a whole.On this dataset, I got a mean test accuracy of 0.8716666666666667 as seen below which is better than the Logistic regression. It needed more training than the LR model(Logistic regression) understandably because this model needs sufficient learning to adjust weights and parameters for the nodes in the hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow NN\n",
    "\n",
    "class ShallowNN(BaseEstimator):\n",
    "    def __init__(self,alpha=0.05,noOfNodes=2,activation='sigmoid',iterations=1000,epsilon = 1e-06): # some parameters can be set\n",
    "        self.noOfNodes = noOfNodes                                                  # set the number of nodes in the hidden layer\n",
    "        self.activation = activation                                                # set the activation\n",
    "        self.nodesList = []                                                         # initialise a list of nodes\n",
    "        self.X = []                                                                 # initialise X values               \n",
    "        self.y = []                                                                 # initialise y values\n",
    "        self.W = []\n",
    "        self.iterations = iterations                                                # initialise weight values\n",
    "        self.b = []                                                                 # initialise bias values\n",
    "        self.alpha = alpha                                                          # initialise learning factor\n",
    "        self.class_labels = {}\n",
    "        self.epsilon = epsilon                                                      # initialise a value for epsilon\n",
    "    \n",
    "    def sigmoid(self,z): # code for the sigmoid function\n",
    "      \"\"\"This function returns the sigmoid value of the input z\"\"\"\n",
    "      return (1/(1+np.exp(-z)))\n",
    "    \n",
    "    def relu(self,z):\n",
    "      \"\"\"This function returns the sigmoid value of the input z\"\"\"  \n",
    "      if abs(z) >= 0:\n",
    "        return z\n",
    "      else:\n",
    "        return np.zeros(z)\n",
    "        \n",
    "    def backProp(self,x,y,a2,a1,z1,z2):\n",
    "        del_z2 = a2-y                                                         # del is the partial derivative                                                         \n",
    "        del_b2 =  del_z2                                                      # calculate del_b2, the partial derivative of bias for the output layer\n",
    "        del_w2 = del_z2 * a1                                                  # calculate del_w2, the partial derivative of weights in the output layer   \n",
    "        fz1 = self.sigmoid(z1)                                                # calculate sigmoid of z1, the linear combination from the first layer\n",
    "        f_z1 = fz1*(1-fz1)                                                    # calculate the derivative of z1\n",
    "        #print(f_z1)\n",
    "        #print(np.dot(del_z2,self.W))\n",
    "        del_z1 = f_z1 * np.dot(del_z2,self.W)                                 # calculate del_z1, the partial derivative for z1 wrt loss\n",
    "        del_w1 = del_z1 * np.expand_dims(x,axis=1)                            # calculate del_w1, the partial derivative of weights in the first layer   \n",
    "        del_b1 = del_z1                                                       # calculate del_b1, the partial derivative of bias in the first layer   \n",
    "        self.W -= self.alpha * del_w2                                         # gradient descent step\n",
    "        self.b -= self.alpha * del_b2                                         # gradient descent step\n",
    "        for i in range(self.noOfNodes):                                       # do this for all the nodes in the hidden layer\n",
    "           node = self.nodesList[i]\n",
    "           node.W -= self.alpha * del_w1[:,i]                                 # gradient descent step\n",
    "           node.b -= self.alpha * del_b1[i]                                   # gradient descent step\n",
    "    \n",
    "    def forwardProp(self,x):\n",
    "        a1 = []                                                               # intialise values\n",
    "        z1 = []                                                               # intialise values\n",
    "        for i in range(self.noOfNodes):                                       # for all the nodes in the hidden layer\n",
    "            node = self.nodesList[i]\n",
    "            z = np.dot(x,node.W.T) + node.b                                   # calculate the linear combination x.W + b\n",
    "            z1.append(z)                                                      # store the value\n",
    "            activateNode = self.sigmoid(z)                                    # calculate the activation for the hidden layer\n",
    "            a1.append(activateNode)                                           # store the activation value\n",
    "        a1 = np.array(a1)\n",
    "        z2 = np.dot(self.W,a1) + self.b                                       # calculate the linear combination a1.W + b\n",
    "        a2 = self.sigmoid(z2)                                                 # calculate the prediction\n",
    "        return a1,a2,np.array(z1),z2\n",
    "    \n",
    "    def loss(self,y,y_):\n",
    "        return (-np.dot(y,np.log(y_.T+self.epsilon)) - np.dot((1-y),np.log((1-y_+self.epsilon).T)))  #return loss\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "      \"\"\"This function trains the NN\"\"\"\n",
    "      self.X = X                                                             # save X\n",
    "      self.convertLabels(list(set(y)))                                       # store distinct values of y and convert them to class numbers\n",
    "      self.y = self.encodeLabels(y)                                          # encode values of y  \n",
    "      rows,columns = X.shape\n",
    "      mu,sigma = 0,0.01                                                      # take mean=0, standard deviation = 0.01\n",
    "      seed = 1                                                               # initialise seed\n",
    "      self.W = np.random.normal(mu,sigma,size=self.noOfNodes)                # choose intial weight values\n",
    "      self.b = np.random.random()                                            # choose intial bias values\n",
    "      \n",
    "      for i in range(self.noOfNodes):\n",
    "          W_node = np.random.normal(mu,sigma,size=columns)                   # start from random values for weights\n",
    "          b_node = np.random.random()                                        # choose intial bias values\n",
    "          lr = LogisticRegression()                                          # use the model from part 1\n",
    "          lr.W = W_node                                                      # save the weights\n",
    "          lr.b = b_node                                                      # save the bias \n",
    "          self.nodesList.append(lr)                                          # store to list \n",
    "      for i in range(self.iterations):\n",
    "          sample = np.random.randint(len(X))                                 # choose a random sample (its SGD)\n",
    "          a1,a2,z1,z2 = self.forwardProp(X[sample])                          # do forward propagation\n",
    "          if (i%10000 == 0):                                                 # check the loss value at intervals of 10000  \n",
    "              lossValue = self.loss(self.y[sample],a2)\n",
    "              print('loss is:',lossValue)\n",
    "          self.backProp(X[sample],self.y[sample],a2,a1,z1,z2)                # do back propagation\n",
    "    \n",
    "    def convertLabels(self,labels):\n",
    "        self.class_labels = { 0 : labels[0] , 1 : labels[1]}                 # convert class values to numbers \n",
    "    \n",
    "    def encodeLabels(self,y):\n",
    "        y_ = []\n",
    "        for i in range(len(y)):                                              # encode labels as 1 or 0\n",
    "            if y[i] == self.class_labels[0]:\n",
    "                y_.append(0)\n",
    "            else:\n",
    "                y_.append(1)\n",
    "        return y_        \n",
    "    \n",
    "    def predict(self,X):\n",
    "        a1,a2,z1,z2 = self.forwardProp(X)                                   # use forward propagation to get the prediction\n",
    "        #print('prediction is',a2)\n",
    "        if a2 >= 0.5:\n",
    "            return self.class_labels[1]                                     # return the highest class\n",
    "        else:  \n",
    "            return self.class_labels[0]                                     # return the lowest class\n",
    "        \n",
    "    def score(self,X,y): # function which returns the accuracy value\n",
    "      \"\"\"This function returns the accuracy score for the X values and compares them with output y.\"\"\"\n",
    "      count = 0  \n",
    "      for x,y_ in zip(X,y):\n",
    "        predictedLabel = self.predict(x)                                   # get the prediction\n",
    "        if predictedLabel == y_:                                           # check if it matches actual\n",
    "            count += 1                                                     # increase the count if True\n",
    "      return count/len(y)                                                  # calculate accuracy    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is  0.8833333333333333\n",
      "The accuracy is  0.8833333333333333\n",
      "The accuracy is  0.9\n",
      "The accuracy is  0.8166666666666667\n",
      "The accuracy is  0.8666666666666667\n",
      "The accuracy is  0.85\n",
      "The accuracy is  0.9166666666666666\n",
      "The accuracy is  0.85\n",
      "The accuracy is  0.9\n",
      "The accuracy is  0.85\n",
      "The average accuracy is 0.8716666666666667\n"
     ]
    }
   ],
   "source": [
    "# Test Moons data for the shallow NN\n",
    "df1 = pd.read_csv('moons400.csv',sep=',')\n",
    "features = df1.drop(columns=['Class'])\n",
    "classes = df1['Class']\n",
    "X = np.array(features)\n",
    "y = np.array(classes)\n",
    "accuracyValues = []\n",
    "parameters = {'alpha' : [1e-3,1e-2,5e-3,5e-2],'noOfNodes':[2,3,5,7,9],'iterations':[1000,2000,5000,10000]}\n",
    "#gs = GridSearchCV(estimator=ShallowNN(),param_grid=parameters)\n",
    "#gs.fit(X,y)\n",
    "#print(gs.best_estimator_)\n",
    "for i in range(10): # use 10 iterations as required\n",
    "     X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=i) # use 15 percent of data as the test set\n",
    "     X_val,X_test,y_val,y_test = train_test_split(X_test, y_test,test_size=0.5,random_state=i)\n",
    "     sn = ShallowNN(noOfNodes=9,iterations=10000) # create an object\n",
    "     sn.fit(X_train,y_train) # train the model\n",
    "     score = sn.score(X_test,y_test)\n",
    "     print('The accuracy is ', score) # get the accuracy/score of the given inputs\n",
    "     accuracyValues.append(score)\n",
    "print('The average accuracy is',np.mean(accuracyValues))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The accuracy is  1.0\n",
      "The average accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test blobs data for the shallow NN\n",
    "df2 = pd.read_csv('blobs250.csv',sep=',')\n",
    "features = df2.drop(columns=['Class'])\n",
    "classes = df2['Class']\n",
    "X = np.array(features)\n",
    "y = np.array(classes)\n",
    "accuracyValues = []\n",
    "parameters = {'alpha' : [1e-3,1e-2,5e-3,5e-2],'noOfNodes':[2,3,5,7,9]}\n",
    "#gs = GridSearchCV(estimator=ShallowNN(),param_grid=parameters)\n",
    "#gs.fit(X,y)\n",
    "#print(gs.best_estimator_)\n",
    "for i in range(10): # use 10 iterations as required\n",
    "   X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=i) # use 1/3 rd of data as the test set\n",
    "   X_val,X_test,y_val,y_test = train_test_split(X_test, y_test,test_size=0.5,random_state=i)\n",
    "   sn = ShallowNN(alpha=0.01) # create an object\n",
    "   sn.fit(X_train,y_train) # train the model\n",
    "   score = sn.score(X_test,y_test)\n",
    "   print('The accuracy is ', score) # get the accuracy/score of the given inputs\n",
    "   accuracyValues.append(score)\n",
    "print('The average accuracy is',np.mean(accuracyValues)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Challenging task\n",
    "\n",
    "Results and observations:\n",
    "For this task, I used a Shallow NN with a hidden layer of 50 nodes. Every image in the chosen batch had a data of length 3072 (1024 * 3 colour channels). I filtered out the classes given to me and used the red channel of the image only. After this, I normalized the data and did a train,test,validate split.\n",
    "\n",
    "I got a test accuracy score of 0.8888888888888888 for 200 epochs.The training times was considerably higher because the hidden layer had significantly more nodes than the ShallowNN model used for the moons dataset.I tried for different combinations of nodes in the hidden layer and the number of epochs and I realized that this model will need sufficient training to be a good enough image classifier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the batch is 4\n",
      "All keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "size of data in this batch: 10000 , size of labels: 10000\n",
      "<class 'numpy.ndarray'>\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# This function taken from the CIFAR website\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "#   data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. \n",
    "#           The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "#           The image is stored in row-major order, so that the first 32 entries of the array are the red channel values \n",
    "#           of the first row of the image.\n",
    "#   labels -- a list of 10000 numbers in the range 0-9. \n",
    "#             The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n",
    "def visualise(data, index):\n",
    "    # MM Jan 2019: Given a CIFAR data nparray and the index of an image, display the image.\n",
    "    # Note that the images will be quite fuzzy looking, because they are low res (32x32).\n",
    "\n",
    "    picture = data[index]\n",
    "    # Initially, the data is a 1D array of 3072 pixels; reshape it to a 3D array of 3x32x32 pixels\n",
    "    # Note: after reshaping like this, you could select one colour channel or average them.\n",
    "    picture.shape = (3,32,32) \n",
    "    \n",
    "    # Plot.imshow requires the RGB to be the third dimension, not the first, so need to rearrange\n",
    "    picture = picture.transpose([1, 2, 0])\n",
    "    plt.imshow(picture)\n",
    "    plt.show()\n",
    "    \n",
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())\n",
    "\n",
    "data = batch1[b'data']\n",
    "labels = batch1[b'labels']\n",
    "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
    "print (type(data))\n",
    "print(data.shape)\n",
    "\n",
    "names = loadlabelnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few images from the batch\n",
    "cifar_dataset = []\n",
    "cifar_classes = []\n",
    "for i in range (len(data)):\n",
    "   if (names[labels[i]] == b'ship') or (names[labels[i]] == b'dog'): # filter the images belonging to the classes given to me for this task (ship,dog)       \n",
    "       img = np.resize(data[i],(3,1024))                             # reshape according to rgb channel values\n",
    "       img = img[0]/255                                              # choose one colour channel and normalize it\n",
    "       cifar_dataset.append(img)                                     # save the image to list\n",
    "       cifar_classes.append(names[labels[i]])                        # save the label of the above image to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CIFAR data for the shallow NN\n",
    "\n",
    "X = np.array(cifar_dataset)            # create a numpy version of the cifar data\n",
    "y = np.array(cifar_classes)            # create a numpy version for the classes\n",
    "\n",
    "accuracyValues = []\n",
    "parameters = {'alpha' : [1e-3,1e-2,5e-3,5e-2],'noOfNodes':[50,150,200]}\n",
    "#gs = GridSearchCV(estimator=ShallowNN(),param_grid=parameters)    # do a grid search\n",
    "#gs.fit(X,y)                                                       # fit to data\n",
    "#print(gs.best_estimator_)                                         # check the best model\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=1) # use 1/3 rd of data as the test set\n",
    "X_val,X_test,y_val,y_test = train_test_split(X, y,test_size=0.5,random_state=1)\n",
    "sn = ShallowNN(alpha=0.01, iterations=len(y_train) * 200, noOfNodes=50) # create an object\n",
    "sn.fit(X_train,y_train) # train the model\n",
    "score = sn.score(X_test,y_test)\n",
    "print('The accuracy is ', score) # get the accuracy/score of the given inputs\n",
    "accuracyValues.append(score)\n",
    "print('The average accuracy is',np.mean(accuracyValues))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sn.score(X_test,y_test)                            \n",
    "with open('ShallowNN.pkl', 'wb') as f:              #save model to disk\n",
    "    pickle.dump(sn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "with open('ShallowNN.pkl', 'rb') as f:             #read model from disk\n",
    "    sn = pickle.load(f)\n",
    "print(sn.score(X_test,y_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Deep Learning Enhancements\n",
    "\n",
    "Results and observations:\n",
    "I implemented Backprop with Momentum as a Deep Learning Enhancement. The fundamental idea [3] to the idea of 'Momentum' is that the change in parameters (or gradients) must be influenced by the previous changes to the same parameters only.This allows the individual parameters to change at a rate appropriate to itself. This is achieved by computing an exponential moving average of previous gradients and using that to update the current parameter.\n",
    "\n",
    "So, when momentum is introduced to back propagation, the below changes apply:\n",
    "VŒîW = (1 ‚àí Œ≤) ŒîW + Œ≤ VŒîW (the current value of VŒîW is based on an exponential moving average) (for weights)\n",
    "VŒîb = (1 ‚àí Œ≤) Œîb + Œ≤ VŒîb (for biases)\n",
    "\n",
    "W ‚àí= Œ±VŒîW , b ‚àí= Œ±VŒîb (Gradient descent update)\n",
    "\n",
    "Studies have shown that Momentum helps the model learn quickly than just batch gradient and stochastic gradient descent.\n",
    "\n",
    "\n",
    "Results:\n",
    "I got a test accuracy score of 0.8878695208970439 for 200 epochs which is similar to the results of the ShallowNN model .The training times was considerably higher than the ShallowNN model used for the moons dataset because the hidden layer had significantly more nodes. With momentum,it helps the model train quicker than SGD and I feel it is a good enhancement to have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow NN with Deep Learning Enhancement\n",
    "\n",
    "class ShallowNNWithMomentum(BaseEstimator):\n",
    "    def __init__(self,alpha=0.05,noOfNodes=2,activation='sigmoid',iterations=1000,epsilon = 1e-06,beta = 0.9):\n",
    "        self.noOfNodes = noOfNodes\n",
    "        self.activation = activation\n",
    "        self.nodesList = []\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.W = []\n",
    "        self.v_delW = []\n",
    "        self.v_delB = []\n",
    "        self.iterations = iterations\n",
    "        self.b = []\n",
    "        self.alpha = alpha\n",
    "        self.class_labels = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "    \n",
    "    def sigmoid(self,z): # code for the sigmoid function\n",
    "      \"\"\"This function returns the sigmoid value of the input z\"\"\"\n",
    "      return (1/(1+np.exp(-z)))\n",
    "    \n",
    "    def relu(self,z):\n",
    "      \"\"\"This function returns the sigmoid value of the input z\"\"\"  \n",
    "      if abs(z)>=0:\n",
    "        return z\n",
    "      else:\n",
    "        return np.zeros(z)\n",
    "        \n",
    "    def backProp(self,x,y,a2,a1,z1,z2,initial=False):\n",
    "        del_z2 = a2-y\n",
    "        del_b2 =  del_z2\n",
    "        del_w2 = np.dot(del_z2,a1.T)\n",
    "        fz1 = self.sigmoid(z1)\n",
    "        f_z1 = fz1*(1-fz1)\n",
    "        del_z1 = f_z1 * np.dot(del_z2,self.W)\n",
    "        del_w1 = del_z1 * np.expand_dims(x,axis=1)\n",
    "        del_b1 = del_z1\n",
    "        if initial:                                                          \n",
    "            self.v_delW = del_w2                                           # intitially vdelW = delW\n",
    "            self.v_delb = del_b2                                           # intitially vdelb = delb\n",
    "        else:\n",
    "            self.v_delW = (1-self.beta) * del_w2 + self.beta * self.v_delW\n",
    "            self.v_delb = (1-self.beta) * del_b2 + self.beta * self.v_delb\n",
    "        self.W -= self.alpha * self.v_delW\n",
    "        self.b -= self.alpha * self.v_delb\n",
    "        \n",
    "        for i in range(self.noOfNodes):\n",
    "           node = self.nodesList[i]\n",
    "           if initial:\n",
    "              node.v_delW = del_w1[:,i]                                   # intitially vdelW = delw\n",
    "              node.v_delb = del_b1[i]                                     # intitially vdelb = delb\n",
    "           else:\n",
    "              node.v_delW = (1-self.beta) * del_w1[:,i] + self.beta * node.v_delW\n",
    "              node.v_delb = (1-self.beta) * del_b1[i] + self.beta * node.v_delb\n",
    "           node.W -= self.alpha * node.v_delW\n",
    "           node.b -= self.alpha * node.v_delb     \n",
    "    \n",
    "    def forwardProp(self,x):\n",
    "        a1 = []\n",
    "        z1 = []\n",
    "        for i in range(self.noOfNodes):\n",
    "            node = self.nodesList[i]\n",
    "            z = np.dot(x,node.W.T) + node.b\n",
    "            z1.append(z)\n",
    "            activateNode = self.sigmoid(z)\n",
    "            a1.append(activateNode)\n",
    "        a1 = np.array(a1)\n",
    "        z2 = np.dot(self.W,a1) + self.b\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a1,a2,np.array(z1),z2\n",
    "    \n",
    "    def loss(self,y,y_):\n",
    "        return (-np.dot(y,np.log(y_.T+self.epsilon)) - np.dot((1-y),np.log((1-y_+self.epsilon).T)))\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "      \"\"\"This function trains the NN\"\"\"\n",
    "      self.X = X\n",
    "      self.convertLabels(list(set(y)))  \n",
    "      self.y = self.encodeLabels(y)\n",
    "      rows,columns = X.shape\n",
    "      mu,sigma = 0,0.01\n",
    "      seed = 1  \n",
    "      self.W = np.random.normal(mu,sigma,size=self.noOfNodes)\n",
    "      self.b = np.random.random()  \n",
    "      #loss = 0\n",
    "        \n",
    "      for i in range(self.noOfNodes):\n",
    "          rows,columns = X.shape\n",
    "          W_node = np.random.normal(mu,sigma,size=columns) # start from random values for weights\n",
    "          #b_node = float(np.random.normal(mu,sigma,size=1))  \n",
    "          b_node = np.random.random()\n",
    "          lr = LogisticRegression()\n",
    "          lr.W = W_node\n",
    "          lr.b = b_node\n",
    "          self.nodesList.append(lr)\n",
    "      for i in range(self.iterations):\n",
    "          sample = np.random.randint(len(X))\n",
    "          a1,a2,z1,z2 = self.forwardProp(X[sample])\n",
    "          #print('a2 is',a1,a2,z1,z2)  \n",
    "          lossValue = self.loss(self.y[sample],a2)\n",
    "          if i%10000 == 0:\n",
    "            print('loss is:',lossValue)\n",
    "          if i==0:\n",
    "             self.backProp(X[sample],self.y[sample],a2,a1,z1,z2,True)\n",
    "          else:      \n",
    "             self.backProp(X[sample],self.y[sample],a2,a1,z1,z2)  \n",
    "    \n",
    "    def convertLabels(self,labels):\n",
    "        self.class_labels = { 0 : labels[0] , 1 : labels[1]}\n",
    "    \n",
    "    def encodeLabels(self,y):\n",
    "        y_ = []\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == self.class_labels[0]:\n",
    "                y_.append(0)\n",
    "            else:\n",
    "                y_.append(1)\n",
    "        return y_        \n",
    "    \n",
    "    def predict(self,X):\n",
    "        a1,a2,z1,z2 = self.forwardProp(X)\n",
    "        #print('prediction is',a2)\n",
    "        if a2 >= 0.5:\n",
    "            return self.class_labels[1]\n",
    "        else:\n",
    "            return self.class_labels[0]\n",
    "        \n",
    "    def score(self,X,y): # function which returns the accuracy value\n",
    "      \"\"\"This function returns the accuracy score for the X values and compares them with output y.\"\"\"\n",
    "      count = 0  \n",
    "      for x,y_ in zip(X,y):\n",
    "        predictedLabel = self.predict(x)\n",
    "        #print('Predicted is',predictedLabel)\n",
    "        #print('Actual is',y_)\n",
    "        if predictedLabel == y_:\n",
    "            count += 1\n",
    "      return count/len(y) # calculate accuracy    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CIFAR data for the shallow NN with momentum\n",
    "\n",
    "X = np.array(cifar_dataset)\n",
    "y = np.array(cifar_classes)\n",
    "\n",
    "accuracyValues = []\n",
    "parameters = {'alpha' : [1e-3,1e-2,5e-3,5e-2],'noOfNodes':[50,100,150]}\n",
    "#gs = GridSearchCV(estimator=ShallowNNWithMomentum(),param_grid=parameters)\n",
    "#gs.fit(X,y)\n",
    "#print(gs.best_estimator_)\n",
    "#for i in range(1): # use 10 iterations as required\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=1) # use 1/3 rd of data as the test set\n",
    "X_val,X_test,y_val,y_test = train_test_split(X, y,test_size=0.5,random_state=1)\n",
    "snw = ShallowNNWithMomentum(alpha=0.01, iterations=200 * len(y_train), noOfNodes=50) # create an object\n",
    "snw.fit(X_train,y_train) # train the model\n",
    "score = sn.score(X_train,y_train)\n",
    "print('The accuracy is ', score) # get the accuracy/score of the given inputs\n",
    "accuracyValues.append(score)\n",
    "print('The average accuracy is',np.mean(accuracyValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8878695208970439\n"
     ]
    }
   ],
   "source": [
    "print(snw.score(X_test,y_test))\n",
    "with open('ShallowNNWithMomentum.pkl', 'wb') as f:          #save model to disk\n",
    "    pickle.dump(snw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8878695208970439"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('ShallowNNWithMomentum.pkl', 'rb') as f:           #read model from disk\n",
    "    snw = pickle.load(f)\n",
    "snw.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1) Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
